{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD3GQk2yg4xu",
        "outputId": "bb72da68-32e2-4c16-ac97-8adbb289b1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "UEbU9FZHhNSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_text=open('sentence_seg.txt','r', encoding=\"utf-8\")\n",
        "my_text=my_text.readline()\n",
        "nltk.download('punkt_tab')\n",
        "my_sentences = sent_tokenize(my_text)\n",
        "print(my_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0SpfzARhS0p",
        "outputId": "edb3a762-b408-4bf9-9283-47293c446651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.', 'If we were asked to build such an application, think about how we would approach doing so at our organization.', 'We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.', 'Since language processing is involved, we would also list all the forms of text processing needed at each step.', 'This step-by-step processing of text is known as pipeline.', 'It is the series of steps involved in building any NLP model.', 'These steps are common in every NLP project, so it makes sense to study them in this chapter.', 'Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace.', 'Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process.', 'In this chapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we’ll see a few guidelines about when and how to use which step.', 'In later chapters, we’ll discuss specific pipelines for various NLP tasks (e.g., Chapters 4–7).']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "DvfuXrawjgDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_text=open('sentence_seg.txt','r', encoding=\"utf-8\")\n",
        "my_text=my_text.readline()\n",
        "word_tokens=word_tokenize(my_text)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-26SZs2ZjjMo",
        "outputId": "24ae7e7e-2070-4f08-fe16-322de56af17d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'previous', 'chapter', ',', 'we', 'saw', 'examples', 'of', 'some', 'common', 'NLP', 'applications', 'that', 'we', 'might', 'encounter', 'in', 'everyday', 'life', '.', 'If', 'we', 'were', 'asked', 'to', 'build', 'such', 'an', 'application', ',', 'think', 'about', 'how', 'we', 'would', 'approach', 'doing', 'so', 'at', 'our', 'organization', '.', 'We', 'would', 'normally', 'walk', 'through', 'the', 'requirements', 'and', 'break', 'the', 'problem', 'down', 'into', 'several', 'sub-problems', ',', 'then', 'try', 'to', 'develop', 'a', 'step-by-step', 'procedure', 'to', 'solve', 'them', '.', 'Since', 'language', 'processing', 'is', 'involved', ',', 'we', 'would', 'also', 'list', 'all', 'the', 'forms', 'of', 'text', 'processing', 'needed', 'at', 'each', 'step', '.', 'This', 'step-by-step', 'processing', 'of', 'text', 'is', 'known', 'as', 'pipeline', '.', 'It', 'is', 'the', 'series', 'of', 'steps', 'involved', 'in', 'building', 'any', 'NLP', 'model', '.', 'These', 'steps', 'are', 'common', 'in', 'every', 'NLP', 'project', ',', 'so', 'it', 'makes', 'sense', 'to', 'study', 'them', 'in', 'this', 'chapter', '.', 'Understanding', 'some', 'common', 'procedures', 'in', 'any', 'NLP', 'pipeline', 'will', 'enable', 'us', 'to', 'get', 'started', 'on', 'any', 'NLP', 'problem', 'encountered', 'in', 'the', 'workplace', '.', 'Laying', 'out', 'and', 'developing', 'a', 'text-processing', 'pipeline', 'is', 'seen', 'as', 'a', 'starting', 'point', 'for', 'any', 'NLP', 'application', 'development', 'process', '.', 'In', 'this', 'chapter', ',', 'we', 'will', 'learn', 'about', 'the', 'various', 'steps', 'involved', 'and', 'how', 'they', 'play', 'important', 'roles', 'in', 'solving', 'the', 'NLP', 'problem', 'and', 'we', '’', 'll', 'see', 'a', 'few', 'guidelines', 'about', 'when', 'and', 'how', 'to', 'use', 'which', 'step', '.', 'In', 'later', 'chapters', ',', 'we', '’', 'll', 'discuss', 'specific', 'pipelines', 'for', 'various', 'NLP', 'tasks', '(', 'e.g.', ',', 'Chapters', '4–7', ')', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "original_text= \"This is a sample sentence, showing off the stop words filtration. We'll be doing a lot more this show. You've heard it.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(original_text)\n",
        "filtered_text = [w for w in word_tokens if w.lower() not in stop_words]\n",
        "print(original_text)\n",
        "print('-----')\n",
        "print(' '.join(filtered_text)) # Changed from filtered_sentence to filtered_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm-ynifrstsR",
        "outputId": "693f30f7-c5ca-437b-be04-6feed6ea3d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sample sentence, showing off the stop words filtration. We'll be doing a lot more this show. You've heard it.\n",
            "-----\n",
            "sample sentence , showing stop words filtration . 'll lot show . 've heard .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words_to_stem = ['car','running', 'jumped', 'revolution','happily', 'quickly', 'foxes']\n",
        "stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n",
        "print(\"Original words:\", words_to_stem)\n",
        "print('-----')\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OenfIb_7vtsY",
        "outputId": "3fa9457e-523b-4add-f737-aba326a41747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['car', 'running', 'jumped', 'revolution', 'happily', 'quickly', 'foxes']\n",
            "-----\n",
            "Stemmed words: ['car', 'run', 'jump', 'revolut', 'happili', 'quickli', 'fox']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fflb3z8OwGvC",
        "outputId": "d7049d2d-ae36-45cf-ca5c-1e3b17d2b58c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "sp= spacy.load(\"en_core_web_sm\")\n",
        "token = sp(u'better and funny')\n",
        "for word in token:\n",
        "    print(word.text, word.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqwmoDFyyT32",
        "outputId": "e4b115c2-cc2f-4e77-9ba6-e3bbacf2d45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "better well\n",
            "and and\n",
            "funny funny\n"
          ]
        }
      ]
    }
  ]
}